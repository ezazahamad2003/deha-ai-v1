import os
import io
import google.generativeai as genai
from pypdf import PdfReader
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Configuration ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY environment variable not set. Please set it before running.")

genai.configure(api_key=GEMINI_API_KEY)

GEMINI_MODEL_NAME = "gemini-1.5-flash-latest"

# --- Load System Prompt from File ---
def load_system_prompt(file_path: str) -> str:
    """Loads the system prompt from a specified file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        raise FileNotFoundError(f"System prompt file not found at: {file_path}")
    except Exception as e:
        raise IOError(f"Error reading system prompt file: {e}")

# Load the system prompt once when the script starts
try:
    SYSTEM_PROMPT_CONTENT = load_system_prompt("system_prompt.txt")
except Exception as e:
    print(f"FATAL ERROR: Could not load system prompt. {e}")
    exit(1)

# --- PDF Text Extraction Function ---
def extract_text_from_pdf(pdf_bytes: bytes) -> str:
    """
    Extracts text from a PDF file provided as bytes.
    Assumes the PDF has selectable text. OCR might be needed for scanned PDFs.
    """
    try:
        pdf_file = io.BytesIO(pdf_bytes)
        reader = PdfReader(pdf_file)
        text = ""
        for page in reader.pages:
            extracted_page_text = page.extract_text()
            if extracted_page_text:
                text += extracted_page_text + "\n"
        return text
    except Exception as e:
        print(f"Error extracting text from PDF: {e}")
        return ""

# --- Deha AI Backend Function ---
# This function will now accept and return the chat history
def deha_ai_backend(pdf_bytes: bytes, patient_question: str, chat_history: list) -> tuple[str, list]:
    """
    Processes a patient's medical PDF and question using the Gemini LLM,
    maintaining conversation history.

    Args:
        pdf_bytes: The content of the PDF file as bytes.
        patient_question: The question posed by the patient.
        chat_history: The list of previous conversation turns.

    Returns:
        A tuple containing:
        - The answer generated by the Gemini LLM.
        - The updated chat_history list.
    """
    patient_pdf_content = extract_text_from_pdf(pdf_bytes)

    if not patient_pdf_content.strip():
        return "I could not extract any meaningful text from the provided PDF. Please ensure it's a valid PDF with selectable text, not just scanned images. For accurate assistance, a clear text-based medical record is needed.", chat_history

    # The PDF content and system prompt are always provided at the start of each turn
    # for consistent context.
    # We combine the system prompt and PDF content into the initial "user" message of the context.
    # Then append the actual question to the chat history for the current turn.

    # Gemini's `GenerativeModel.start_chat` automatically manages context for you.
    # You feed it the system prompt and context first.
    # Then you just send user questions.

    # The initial message for the chat should contain the system prompt and PDF content
    initial_context_message = f"""
    {SYSTEM_PROMPT_CONTENT}

    Here is the extracted text from the patient's medical PDF:
    <PDF_START>
    {patient_pdf_content}
    <PDF_END>
    """

    try:
        # Initialize the model and start a chat session.
        # If chat_history is empty, it means we are starting a new conversation.
        # If it's not empty, we are resuming.
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        
        # If this is the very first turn, the chat_history will be empty.
        # We need to initialize the chat with the system prompt + PDF content.
        if not chat_history:
            chat_session = model.start_chat(history=[
                {"role": "user", "parts": [initial_context_message]}
                # Note: The model's initial response to this first "user" message
                # will be empty or a greeting, which we might want to skip or handle.
                # For simplicity here, we'll let it respond and add it to history.
                # A more advanced setup might send an empty initial message for the model
                # or have a dedicated 'initialization' function.
            ])
            # Add the model's empty (or greeting) response to history if it has one
            # to keep the turn balanced.
            # This is a bit tricky: `start_chat` doesn't generate content immediately.
            # It just sets up the history. We will immediately send the user's question.
        else:
            # If chat_history exists, re-initialize the chat session with the existing history.
            # The system prompt and PDF content should ideally be part of the *initial*
            # context that was added to `history` when `start_chat` was first called.
            # For this simple implementation, we'll just continue passing the history.
            # A more robust solution might handle the system prompt as a separate parameter
            # in the chat session setup if the API supports it more cleanly.
            # For gemini-1.5-flash, the prompt is part of the first 'user' turn.
            chat_session = model.start_chat(history=chat_history)
        
        # Send the patient's current question
        response = chat_session.send_message(
            content=patient_question,
            generation_config=genai.types.GenerationConfig(
                temperature=0.7,
                top_p=0.95,
                top_k=40,
                max_output_tokens=1024
            )
        )
        
        llm_answer = response.text
        
        # Update the chat history with the current turn
        # Gemini's chat_session.history automatically includes the last user and model messages
        # So we just retrieve the updated history from the session.
        updated_chat_history = chat_session.history
        
        return llm_answer, updated_chat_history

    except Exception as e:
        print(f"Error calling Gemini LLM: {e}")
        return "I apologize, but an unexpected error occurred while processing your request. Please try again later. For any medical concerns, please consult a healthcare professional.", chat_history

# --- Interactive CLI Usage ---
if __name__ == "__main__":
    print("Welcome to Deha AI! Your personal medical information assistant.")

    pdf_bytes_for_session = b""
    chat_history = [] # Initialize chat history here

    while True:
        pdf_file_path = input("\nPlease enter the path to your medical PDF file (e.g., medical_record.pdf) or type 'exit' to quit: ").strip()

        if pdf_file_path.lower() == 'exit':
            print("Exiting Deha AI. Goodbye!")
            break

        if not pdf_file_path:
            print("No path entered. Please try again.")
            continue

        try:
            with open(pdf_file_path, "rb") as f:
                pdf_bytes_for_session = f.read()
            print(f"Successfully loaded PDF from: '{pdf_file_path}'")
            
            # Initialize the chat history for the *new session* with the system prompt and PDF content
            # This ensures the LLM has context from the start of the conversation.
            initial_context_message = f"""
            {SYSTEM_PROMPT_CONTENT}

            Here is the extracted text from the patient's medical PDF:
            <PDF_START>
            {extract_text_from_pdf(pdf_bytes_for_session)}
            <PDF_END>
            """
            
            # Start a chat session with the initial context
            # We explicitly define the model here for the chat session init
            model_for_chat_init = genai.GenerativeModel(GEMINI_MODEL_NAME)
            chat_session_initializer = model_for_chat_init.start_chat(history=[])
            
            # Send the initial context as the first user message
            initial_response = chat_session_initializer.send_message(initial_context_message)
            # The actual response might be empty or a greeting, which is fine for context setting
            
            chat_history = chat_session_initializer.history # Capture the initial context in history
            
            break # Exit the PDF input loop once successful
        except FileNotFoundError:
            print(f"Error: PDF file not found at '{pdf_file_path}'. Please check the path and try again.")
        except Exception as e:
            print(f"An unexpected error occurred while reading the PDF: {e}")

    if not pdf_bytes_for_session:
        print("No PDF loaded. Deha AI cannot function without your medical record.")
        exit()

    print("\nPDF processed. You can now ask questions about your medical record or general health information.")
    print("Type 'quit' or 'exit' to end the conversation.")

    while True:
        user_question = input("\nYour question: ").strip()

        if user_question.lower() in ['quit', 'exit']:
            print("Exiting Deha AI. Goodbye!")
            break
        if not user_question:
            print("Please enter a question.")
            continue

        # Pass the current chat_history and update it with the new turn
        answer, chat_history = deha_ai_backend(pdf_bytes_for_session, user_question, chat_history)
        
        print(f"\nDeha AI's Answer:\n{answer}")
        print("\n" + "="*70 + "\n")