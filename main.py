import os
import io
import google.generativeai as genai
from pypdf import PdfReader
from dotenv import load_dotenv # Import load_dotenv

# Load environment variables from .env file
load_dotenv() # Add this line to load variables

# --- Configuration ---
# IMPORTANT: Replace with your actual Gemini API key.
# It's best practice to load this from environment variables.
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY environment variable not set. Please set it before running.")

genai.configure(api_key=GEMINI_API_KEY)

# Choose a suitable Gemini model
GEMINI_MODEL_NAME = "gemini-1.5-flash-latest"

# --- Load System Prompt from File ---
def load_system_prompt(file_path: str) -> str:
    """Loads the system prompt from a specified file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        raise FileNotFoundError(f"System prompt file not found at: {file_path}")
    except Exception as e:
        raise IOError(f"Error reading system prompt file: {e}")

# Load the system prompt once when the script starts
try:
    SYSTEM_PROMPT_CONTENT = load_system_prompt("system_prompt.txt")
except Exception as e:
    print(f"FATAL ERROR: Could not load system prompt. {e}")
    exit(1) # Exit if the system prompt can't be loaded

# --- PDF Text Extraction Function ---
def extract_text_from_pdf(pdf_bytes: bytes) -> str:
    """
    Extracts text from a PDF file provided as bytes.
    Assumes the PDF has selectable text. OCR might be needed for scanned PDFs.
    """
    try:
        pdf_file = io.BytesIO(pdf_bytes)
        reader = PdfReader(pdf_file)
        text = ""
        for page in reader.pages:
            extracted_page_text = page.extract_text()
            if extracted_page_text:
                text += extracted_page_text + "\n"
        return text
    except Exception as e:
        print(f"Error extracting text from PDF: {e}")
        return ""

# --- Deha AI Backend Function ---
def deha_ai_backend(pdf_bytes: bytes, patient_question: str) -> str:
    """
    Processes a patient's medical PDF and question using the Gemini LLM.

    Args:
        pdf_bytes: The content of the PDF file as bytes.
        patient_question: The question posed by the patient.

    Returns:
        The answer generated by the Gemini LLM.
    """
    # No more debug prints here!
    
    # 1. Extract text from PDF
    patient_pdf_content = extract_text_from_pdf(pdf_bytes)

    if not patient_pdf_content.strip():
        return "I could not extract any meaningful text from the provided PDF. Please ensure it's a valid PDF with selectable text, not just scanned images. For accurate assistance, a clear text-based medical record is needed."

    # 2. Construct the User Input using the loaded system prompt
    user_content = f"""
    Here is the extracted text from the patient's medical PDF:
    <PDF_START>
    {patient_pdf_content}
    <PDF_END>

    Here is the patient's question:
    <QUESTION_START>
    {patient_question}
    <QUESTION_END>
    """

    # 3. Call the Gemini LLM
    try:
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        response = model.generate_content(
            contents=[
                {"role": "user", "parts": [SYSTEM_PROMPT_CONTENT, user_content]} # Use the loaded prompt
            ],
            generation_config=genai.types.GenerationConfig(
                temperature=0.7,
                top_p=0.95,
                top_k=40,
                max_output_tokens=1024
            )
        )
        llm_answer = response.text
        return llm_answer

    except Exception as e:
        print(f"Error calling Gemini LLM: {e}") # Keep this for internal debugging, not for user.
        return "I apologize, but an unexpected error occurred while processing your request. Please try again later. For any medical concerns, please consult a healthcare professional."

# --- Interactive CLI Usage ---
if __name__ == "__main__":
    print("Welcome to Deha AI! Your personal medical information assistant.")

    pdf_bytes_for_session = b"" # Initialize

    while True:
        pdf_file_path = input("\nPlease enter the path to your medical PDF file (e.g., medical_record.pdf) or type 'exit' to quit: ").strip()

        if pdf_file_path.lower() == 'exit':
            print("Exiting Deha AI. Goodbye!")
            break

        if not pdf_file_path:
            print("No path entered. Please try again.")
            continue

        try:
            with open(pdf_file_path, "rb") as f:
                pdf_bytes_for_session = f.read()
            print(f"Successfully loaded PDF from: '{pdf_file_path}'")
            break
        except FileNotFoundError:
            print(f"Error: PDF file not found at '{pdf_file_path}'. Please check the path and try again.")
        except Exception as e:
            print(f"An unexpected error occurred while reading the PDF: {e}")

    if not pdf_bytes_for_session:
        print("No PDF loaded. Deha AI cannot function without your medical record.")
        exit()

    print("\nPDF processed. You can now ask questions about your medical record or general health information.")
    print("Type 'quit' or 'exit' to end the conversation.")

    while True:
        user_question = input("\nYour question: ").strip()

        if user_question.lower() in ['quit', 'exit']:
            print("Exiting Deha AI. Goodbye!")
            break
        if not user_question:
            print("Please enter a question.")
            continue

        answer = deha_ai_backend(pdf_bytes_for_session, user_question)
        print(f"\nDeha AI's Answer:\n{answer}")
        print("\n" + "="*70 + "\n")