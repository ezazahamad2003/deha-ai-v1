import os
import io
import google.generativeai as genai
from pypdf import PdfReader
from audio import speak, listen
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# --- Configuration ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEY environment variable not set. Please set it before running.")

genai.configure(api_key=GEMINI_API_KEY)

GEMINI_MODEL_NAME = "gemini-1.5-flash-latest"

# --- Load System Prompt from File ---
def load_system_prompt(file_path: str) -> str:
    """Loads the system prompt from a specified file."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        raise FileNotFoundError(f"System prompt file not found at: {file_path}")
    except Exception as e:
        raise IOError(f"Error reading system prompt file: {e}")

# Load the system prompt once when the script starts
try:
    SYSTEM_PROMPT_CONTENT = load_system_prompt("system_prompt.txt")
except Exception as e:
    print(f"FATAL ERROR: Could not load system prompt. {e}")
    exit(1)

# --- PDF Text Extraction Function ---
def extract_text_from_pdf(pdf_bytes: bytes) -> str:
    """
    Extracts text from a PDF file provided as bytes.
    Assumes the PDF has selectable text. OCR might be needed for scanned PDFs.
    """
    try:
        pdf_file = io.BytesIO(pdf_bytes)
        reader = PdfReader(pdf_file)
        text = ""
        for page in reader.pages:
            extracted_page_text = page.extract_text()
            if extracted_page_text:
                text += extracted_page_text + "\n"
        return text
    except Exception as e:
        print(f"Error extracting text from PDF: {e}")
        return ""

# --- Deha AI Backend Function ---
# This function will now accept and return the chat history
def deha_ai_backend(pdf_bytes: bytes, patient_question: str, chat_history: list) -> tuple[str, list]:
    """
    Processes a patient's medical PDF and question using the Gemini LLM,
    maintaining conversation history.

    Args:
        pdf_bytes: The content of the PDF file as bytes.
        patient_question: The question posed by the patient.
        chat_history: The list of previous conversation turns.

    Returns:
        A tuple containing:
        - The answer generated by the Gemini LLM.
        - The updated chat_history list.
    """
    patient_pdf_content = extract_text_from_pdf(pdf_bytes)

    if not patient_pdf_content.strip():
        return "I could not extract any meaningful text from the provided PDF. Please ensure it's a valid PDF with selectable text, not just scanned images. For accurate assistance, a clear text-based medical record is needed.", chat_history

    # The PDF content and system prompt are always provided at the start of each turn
    # for consistent context.
    # We combine the system prompt and PDF content into the initial "user" message of the context.
    # Then append the actual question to the chat history for the current turn.

    # Gemini's `GenerativeModel.start_chat` automatically manages context for you.
    # You feed it the system prompt and context first.
    # Then you just send user questions.

    # The initial message for the chat should contain the system prompt and PDF content
    initial_context_message = f"""
    {SYSTEM_PROMPT_CONTENT}

    Here is the extracted text from the patient's medical PDF:
    <PDF_START>
    {patient_pdf_content}
    <PDF_END>
    """

    try:
        # Initialize the model and start a chat session.
        # If chat_history is empty, it means we are starting a new conversation.
        # If it's not empty, we are resuming.
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        
        # If this is the very first turn, the chat_history will be empty.
        # We need to initialize the chat with the system prompt + PDF content.
        if not chat_history:
            chat_session = model.start_chat(history=[
                {"role": "user", "parts": [initial_context_message]}
                # Note: The model's initial response to this first "user" message
                # will be empty or a greeting, which we might want to skip or handle.
                # For simplicity here, we'll let it respond and add it to history.
                # A more advanced setup might send an empty initial message for the model
                # or have a dedicated 'initialization' function.
            ])
            # Add the model's empty (or greeting) response to history if it has one
            # to keep the turn balanced.
            # This is a bit tricky: `start_chat` doesn't generate content immediately.
            # It just sets up the history. We will immediately send the user's question.
        else:
            # If chat_history exists, re-initialize the chat session with the existing history.
            # The system prompt and PDF content should ideally be part of the *initial*
            # context that was added to `history` when `start_chat` was first called.
            # For this simple implementation, we'll just continue passing the history.
            # A more robust solution might handle the system prompt as a separate parameter
            # in the chat session setup if the API supports it more cleanly.
            # For gemini-1.5-flash, the prompt is part of the first 'user' turn.
            chat_session = model.start_chat(history=chat_history)
        
        # Send the patient's current question
        response = chat_session.send_message(
            content=patient_question,
            generation_config=genai.types.GenerationConfig(
                temperature=0.7,
                top_p=0.95,
                top_k=40,
                max_output_tokens=1024
            )
        )
        
        llm_answer = response.text
        
        # Update the chat history with the current turn
        # Gemini's chat_session.history automatically includes the last user and model messages
        # So we just retrieve the updated history from the session.
        updated_chat_history = chat_session.history
        
        return llm_answer, updated_chat_history

    except Exception as e:
        print(f"Error calling Gemini LLM: {e}") # Keep this for internal debugging
        # Provide a user-friendly message and return the original chat_history
        return "I apologize, but an unexpected error occurred while processing your request. Please try again later. For any medical concerns, please consult a healthcare professional.", chat_history

if __name__ == "__main__":
    print("Welcome to Deha AI! Your personal medical information assistant.")

    pdf_bytes_for_session = b"" # Initialize

    # --- PDF Loading Loop ---
    while True:
        pdf_file_path = input("\nPlease enter the path to your medical PDF file (e.g., medical_record.pdf) or type 'exit' to quit: ").strip()

        if pdf_file_path.lower() == 'exit':
            print("Exiting Deha AI. Goodbye!")
            exit() # Exit the script if user types 'exit' here

        if not pdf_file_path:
            print("  ‚ùó No path entered. Please try again.")
            continue

        try:
            with open(pdf_file_path, "rb") as f:
                pdf_bytes_for_session = f.read()
            print(f"  ‚úÖ Successfully loaded PDF from: '{pdf_file_path}'")
            break # Exit PDF input loop once a valid PDF is loaded
        except FileNotFoundError:
            print(f"  ‚ùå Error: PDF file not found at '{pdf_file_path}'. Please check the path and try again.")
        except Exception as e:
            print(f"  ‚ùå An unexpected error occurred while reading the PDF: {e}")

    # This check is important. If the loop somehow exited without loading a PDF (though current logic should prevent this unless 'exit' is typed).
    if not pdf_bytes_for_session:
        print("No PDF loaded. Deha AI cannot function without your medical record. Exiting.")
        exit()

    # Initialize chat history for the session
    chat_history = []
    
    print("\nPDF processed. You can now ask questions about your medical record or general health information.")
    print("Type 'quit' or 'exit' to end the conversation.\n")

    # --- Choose mode once ---
    mode = ""
    while mode not in ("audio", "text"):
        mode = input("Select interaction mode ('audio' or 'text'): ").strip().lower()
        if mode not in ("audio", "text"):
            print("  ‚ùó Please enter exactly 'audio' or 'text'.")

    # --- Main Q&A loop, inside the same scope where pdf_bytes_for_session exists ---
    while True:
        if mode == "audio":
            print("üé§ Listening...")
            user_question = listen().strip()
        else:
            user_question = input("üí¨ Your question: ").strip()

        # skip empty
        if not user_question:
            if mode == "audio":
                print("   [Audio Info] Didn't catch that.")
                continue
            else:
                print("Please enter a question.")
                continue

        # quit
        if user_question.lower() in ("quit", "exit"):
            bye = "Goodbye! Take care."
            print(f"\nü§ñ Deha AI: {bye}")
            if mode == "audio":
                speak(bye)
            break

        # ‚Äî‚Äî‚Äî HERE is where pdf_bytes_for_session must be in scope ‚Äî‚Äî‚Äî
        answer, chat_history = deha_ai_backend(
            pdf_bytes_for_session,
            user_question,
            chat_history
        )

        # print/speak the answer
        print(f"\nü§ñ Deha AI:\n{answer}\n")
        if mode == "audio":
            speak(answer)

        print("="*60, "\n")
